{
  "provider": "openrouter",
  "provider_info": {
    "name": "OpenRouter",
    "website": "https://openrouter.ai",
    "api_endpoint": "https://openrouter.ai/api/v1"
  },
  "last_updated": "2025-07-17T09:19:10.768699",
  "models": [
    {
      "id": "mistralai/mistral-medium-3",
      "name": "Mistral",
      "provider": "openrouter",
      "description": "Mistral Medium 3 is a high-performance enterprise-grade language model designed to deliver frontier-level capabilities at significantly reduced operational cost. It balances state-of-the-art reasoning and multimodal performance with 8Ã— lower cost compared to traditional large models, making it suitable for scalable deployments across professional and industrial use cases.\n\nThe model excels in domains such as coding, STEM reasoning, and enterprise adaptation. It supports hybrid, on-prem, and in-VPC deployments and is optimized for integration into custom workflows. Mistral Medium 3 offers competitive accuracy relative to larger models like Claude Sonnet 3.5/3.7, Llama 4 Maverick, and Command R+, while maintaining broad compatibility across cloud environments.",
      "pricing": {
        "input": 0.4,
        "output": 2.0,
        "unit": "1M tokens"
      },
      "context_window": 131072,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768538"
    },
    {
      "id": "deepseek/deepseek-chat-v3-0324:free",
      "name": "DeepSeek",
      "provider": "openrouter",
      "description": "DeepSeek V3, a 685B-parameter, mixture-of-experts model, is the latest iteration of the flagship chat model family from the DeepSeek team.\n\nIt succeeds the [DeepSeek V3](/deepseek/deepseek-chat-v3) model and performs really well on a variety of tasks.",
      "pricing": {
        "input": 0.0,
        "output": 0.0,
        "unit": "1M tokens"
      },
      "context_window": 32768,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768548"
    },
    {
      "id": "deepseek/deepseek-chat-v3-0324",
      "name": "DeepSeek",
      "provider": "openrouter",
      "description": "DeepSeek V3, a 685B-parameter, mixture-of-experts model, is the latest iteration of the flagship chat model family from the DeepSeek team.\n\nIt succeeds the [DeepSeek V3](/deepseek/deepseek-chat-v3) model and performs really well on a variety of tasks.",
      "pricing": {
        "input": 0.25,
        "output": 0.85,
        "unit": "1M tokens"
      },
      "context_window": 163840,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768550"
    },
    {
      "id": "openai/gpt-4o-mini-search-preview",
      "name": "OpenAI",
      "provider": "openrouter",
      "description": "GPT-4o mini Search Preview is a specialized model for web search in Chat Completions. It is trained to understand and execute web search queries.",
      "pricing": {
        "input": 0.15,
        "output": 0.6,
        "unit": "1M tokens"
      },
      "context_window": 128000,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768551"
    },
    {
      "id": "openai/gpt-4o-search-preview",
      "name": "OpenAI",
      "provider": "openrouter",
      "description": "GPT-4o Search Previewis a specialized model for web search in Chat Completions. It is trained to understand and execute web search queries.",
      "pricing": {
        "input": 2.5,
        "output": 10.0,
        "unit": "1M tokens"
      },
      "context_window": 128000,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768552"
    },
    {
      "id": "deepseek/deepseek-chat:free",
      "name": "DeepSeek",
      "provider": "openrouter",
      "description": "DeepSeek-V3 is the latest model from the DeepSeek team, building upon the instruction following and coding abilities of the previous versions. Pre-trained on nearly 15 trillion tokens, the reported evaluations reveal that the model outperforms other open-source models and rivals leading closed-source models.\n\nFor model details, please visit [the DeepSeek-V3 repo](https://github.com/deepseek-ai/DeepSeek-V3) for more information, or see the [launch announcement](https://api-docs.deepseek.com/news/news1226).",
      "pricing": {
        "input": 0.0,
        "output": 0.0,
        "unit": "1M tokens"
      },
      "context_window": 163840,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768554"
    },
    {
      "id": "deepseek/deepseek-chat",
      "name": "DeepSeek",
      "provider": "openrouter",
      "description": "DeepSeek-V3 is the latest model from the DeepSeek team, building upon the instruction following and coding abilities of the previous versions. Pre-trained on nearly 15 trillion tokens, the reported evaluations reveal that the model outperforms other open-source models and rivals leading closed-source models.\n\nFor model details, please visit [the DeepSeek-V3 repo](https://github.com/deepseek-ai/DeepSeek-V3) for more information, or see the [launch announcement](https://api-docs.deepseek.com/news/news1226).",
      "pricing": {
        "input": 0.3,
        "output": 0.85,
        "unit": "1M tokens"
      },
      "context_window": 163840,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768555"
    },
    {
      "id": "x-ai/grok-2-vision-1212",
      "name": "xAI",
      "provider": "openrouter",
      "description": "Grok 2 Vision 1212 advances image-based AI with stronger visual comprehension, refined instruction-following, and multilingual support. From object recognition to style analysis, it empowers developers to build more intuitive, visually aware applications. Its enhanced steerability and reasoning establish a robust foundation for next-generation image solutions.\n\nTo read more about this model, check out [xAI's announcement](https://x.ai/blog/grok-1212).",
      "pricing": {
        "input": 2.0,
        "output": 10.0,
        "unit": "1M tokens"
      },
      "context_window": 32768,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768556"
    },
    {
      "id": "x-ai/grok-2-1212",
      "name": "xAI",
      "provider": "openrouter",
      "description": "Grok 2 1212 introduces significant enhancements to accuracy, instruction adherence, and multilingual support, making it a powerful and flexible choice for developers seeking a highly steerable, intelligent model.",
      "pricing": {
        "input": 2.0,
        "output": 10.0,
        "unit": "1M tokens"
      },
      "context_window": 131072,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768557"
    },
    {
      "id": "google/gemini-2.0-flash-exp:free",
      "name": "Google",
      "provider": "openrouter",
      "description": "Gemini Flash 2.0 offers a significantly faster time to first token (TTFT) compared to [Gemini Flash 1.5](/google/gemini-flash-1.5), while maintaining quality on par with larger models like [Gemini Pro 1.5](/google/gemini-pro-1.5). It introduces notable enhancements in multimodal understanding, coding capabilities, complex instruction following, and function calling. These advancements come together to deliver more seamless and robust agentic experiences.",
      "pricing": {
        "input": 0.0,
        "output": 0.0,
        "unit": "1M tokens"
      },
      "context_window": 1048576,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768558"
    },
    {
      "id": "meta-llama/llama-3.3-70b-instruct:free",
      "name": "Meta",
      "provider": "openrouter",
      "description": "The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperforms many of the available open source and closed chat models on common industry benchmarks.\n\nSupported languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\n\n[Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md)",
      "pricing": {
        "input": 0.0,
        "output": 0.0,
        "unit": "1M tokens"
      },
      "context_window": 65536,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768559"
    },
    {
      "id": "meta-llama/llama-3.3-70b-instruct",
      "name": "Meta",
      "provider": "openrouter",
      "description": "The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperforms many of the available open source and closed chat models on common industry benchmarks.\n\nSupported languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\n\n[Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md)",
      "pricing": {
        "input": 0.04,
        "output": 0.12,
        "unit": "1M tokens"
      },
      "context_window": 131072,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768560"
    },
    {
      "id": "openai/gpt-4o-2024-11-20",
      "name": "OpenAI",
      "provider": "openrouter",
      "description": "The 2024-11-20 version of GPT-4o offers a leveled-up creative writing ability with more natural, engaging, and tailored writing to improve relevance & readability. Itâ€™s also better at working with uploaded files, providing deeper insights & more thorough responses.\n\nGPT-4o (\"o\" for \"omni\") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/models/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.",
      "pricing": {
        "input": 2.5,
        "output": 10.0,
        "unit": "1M tokens"
      },
      "context_window": 128000,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768561"
    },
    {
      "id": "mistralai/mistral-large-2411",
      "name": "Mistral Large 2411",
      "provider": "openrouter",
      "description": "Mistral Large 2 2411 is an update of [Mistral Large 2](/mistralai/mistral-large) released together with [Pixtral Large 2411](/mistralai/pixtral-large-2411)\n\nIt provides a significant upgrade on the previous [Mistral Large 24.07](/mistralai/mistral-large-2407), with notable improvements in long context understanding, a new system prompt, and more accurate function calling.",
      "pricing": {
        "input": 2.0,
        "output": 6.0,
        "unit": "1M tokens"
      },
      "context_window": 131072,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768563"
    },
    {
      "id": "mistralai/mistral-large-2407",
      "name": "Mistral Large 2407",
      "provider": "openrouter",
      "description": "This is Mistral AI's flagship model, Mistral Large 2 (version mistral-large-2407). It's a proprietary weights-available model and excels at reasoning, code, JSON, chat, and more. Read the launch announcement [here](https://mistral.ai/news/mistral-large-2407/).\n\nIt supports dozens of languages including French, German, Spanish, Italian, Portuguese, Arabic, Hindi, Russian, Chinese, Japanese, and Korean, along with 80+ coding languages including Python, Java, C, C++, JavaScript, and Bash. Its long context window allows precise information recall from large documents.\n",
      "pricing": {
        "input": 2.0,
        "output": 6.0,
        "unit": "1M tokens"
      },
      "context_window": 131072,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768564"
    },
    {
      "id": "mistralai/pixtral-large-2411",
      "name": "Mistral",
      "provider": "openrouter",
      "description": "Pixtral Large is a 124B parameter, open-weight, multimodal model built on top of [Mistral Large 2](/mistralai/mistral-large-2411). The model is able to understand documents, charts and natural images.\n\nThe model is available under the Mistral Research License (MRL) for research and educational use, and the Mistral Commercial License for experimentation, testing, and production for commercial purposes.\n\n",
      "pricing": {
        "input": 2.0,
        "output": 6.0,
        "unit": "1M tokens"
      },
      "context_window": 131072,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768565"
    },
    {
      "id": "anthropic/claude-3.5-haiku:beta",
      "name": "Anthropic",
      "provider": "openrouter",
      "description": "Claude 3.5 Haiku features offers enhanced capabilities in speed, coding accuracy, and tool use. Engineered to excel in real-time applications, it delivers quick response times that are essential for dynamic tasks such as chat interactions and immediate coding suggestions.\n\nThis makes it highly suitable for environments that demand both speed and precision, such as software development, customer service bots, and data management systems.\n\nThis model is currently pointing to [Claude 3.5 Haiku (2024-10-22)](/anthropic/claude-3-5-haiku-20241022).",
      "pricing": {
        "input": 0.8,
        "output": 4.0,
        "unit": "1M tokens"
      },
      "context_window": 200000,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768566"
    },
    {
      "id": "anthropic/claude-3.5-haiku",
      "name": "Anthropic",
      "provider": "openrouter",
      "description": "Claude 3.5 Haiku features offers enhanced capabilities in speed, coding accuracy, and tool use. Engineered to excel in real-time applications, it delivers quick response times that are essential for dynamic tasks such as chat interactions and immediate coding suggestions.\n\nThis makes it highly suitable for environments that demand both speed and precision, such as software development, customer service bots, and data management systems.\n\nThis model is currently pointing to [Claude 3.5 Haiku (2024-10-22)](/anthropic/claude-3-5-haiku-20241022).",
      "pricing": {
        "input": 0.8,
        "output": 4.0,
        "unit": "1M tokens"
      },
      "context_window": 200000,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768567"
    },
    {
      "id": "anthropic/claude-3.5-haiku-20241022:beta",
      "name": "Anthropic",
      "provider": "openrouter",
      "description": "Claude 3.5 Haiku features enhancements across all skill sets including coding, tool use, and reasoning. As the fastest model in the Anthropic lineup, it offers rapid response times suitable for applications that require high interactivity and low latency, such as user-facing chatbots and on-the-fly code completions. It also excels in specialized tasks like data extraction and real-time content moderation, making it a versatile tool for a broad range of industries.\n\nIt does not support image inputs.\n\nSee the launch announcement and benchmark results [here](https://www.anthropic.com/news/3-5-models-and-computer-use)",
      "pricing": {
        "input": 0.8,
        "output": 4.0,
        "unit": "1M tokens"
      },
      "context_window": 200000,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768568"
    },
    {
      "id": "anthropic/claude-3.5-haiku-20241022",
      "name": "Anthropic",
      "provider": "openrouter",
      "description": "Claude 3.5 Haiku features enhancements across all skill sets including coding, tool use, and reasoning. As the fastest model in the Anthropic lineup, it offers rapid response times suitable for applications that require high interactivity and low latency, such as user-facing chatbots and on-the-fly code completions. It also excels in specialized tasks like data extraction and real-time content moderation, making it a versatile tool for a broad range of industries.\n\nIt does not support image inputs.\n\nSee the launch announcement and benchmark results [here](https://www.anthropic.com/news/3-5-models-and-computer-use)",
      "pricing": {
        "input": 0.8,
        "output": 4.0,
        "unit": "1M tokens"
      },
      "context_window": 200000,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768569"
    },
    {
      "id": "anthropic/claude-3.5-sonnet:beta",
      "name": "Anthropic",
      "provider": "openrouter",
      "description": "New Claude 3.5 Sonnet delivers better-than-Opus capabilities, faster-than-Sonnet speeds, at the same Sonnet prices. Sonnet is particularly good at:\n\n- Coding: Scores ~49% on SWE-Bench Verified, higher than the last best score, and without any fancy prompt scaffolding\n- Data science: Augments human data science expertise; navigates unstructured data while using multiple tools for insights\n- Visual processing: excelling at interpreting charts, graphs, and images, accurately transcribing text to derive insights beyond just the text alone\n- Agentic tasks: exceptional tool use, making it great at agentic tasks (i.e. complex, multi-step problem solving tasks that require engaging with other systems)\n\n#multimodal",
      "pricing": {
        "input": 3.0,
        "output": 15.0,
        "unit": "1M tokens"
      },
      "context_window": 200000,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768570"
    },
    {
      "id": "anthropic/claude-3.5-sonnet",
      "name": "Anthropic",
      "provider": "openrouter",
      "description": "New Claude 3.5 Sonnet delivers better-than-Opus capabilities, faster-than-Sonnet speeds, at the same Sonnet prices. Sonnet is particularly good at:\n\n- Coding: Scores ~49% on SWE-Bench Verified, higher than the last best score, and without any fancy prompt scaffolding\n- Data science: Augments human data science expertise; navigates unstructured data while using multiple tools for insights\n- Visual processing: excelling at interpreting charts, graphs, and images, accurately transcribing text to derive insights beyond just the text alone\n- Agentic tasks: exceptional tool use, making it great at agentic tasks (i.e. complex, multi-step problem solving tasks that require engaging with other systems)\n\n#multimodal",
      "pricing": {
        "input": 3.0,
        "output": 15.0,
        "unit": "1M tokens"
      },
      "context_window": 200000,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768571"
    },
    {
      "id": "nvidia/llama-3.1-nemotron-70b-instruct",
      "name": "NVIDIA",
      "provider": "openrouter",
      "description": "NVIDIA's Llama 3.1 Nemotron 70B is a language model designed for generating precise and useful responses. Leveraging [Llama 3.1 70B](/models/meta-llama/llama-3.1-70b-instruct) architecture and Reinforcement Learning from Human Feedback (RLHF), it excels in automatic alignment benchmarks. This model is tailored for applications requiring high accuracy in helpfulness and response generation, suitable for diverse user queries across multiple domains.\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).",
      "pricing": {
        "input": 0.12,
        "output": 0.3,
        "unit": "1M tokens"
      },
      "context_window": 131072,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768573"
    },
    {
      "id": "meta-llama/llama-3.2-90b-vision-instruct",
      "name": "Meta",
      "provider": "openrouter",
      "description": "The Llama 90B Vision model is a top-tier, 90-billion-parameter multimodal model designed for the most challenging visual reasoning and language tasks. It offers unparalleled accuracy in image captioning, visual question answering, and advanced image-text comprehension. Pre-trained on vast multimodal datasets and fine-tuned with human feedback, the Llama 90B Vision is engineered to handle the most demanding image-based AI tasks.\n\nThis model is perfect for industries requiring cutting-edge multimodal AI capabilities, particularly those dealing with complex, real-time visual and textual analysis.\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).",
      "pricing": {
        "input": 1.2,
        "output": 1.2,
        "unit": "1M tokens"
      },
      "context_window": 131072,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768574"
    },
    {
      "id": "qwen/qwen-2.5-72b-instruct:free",
      "name": "Qwen2.5 72B Instruct (free)",
      "provider": "openrouter",
      "description": "Qwen2.5 72B is the latest series of Qwen large language models. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\n\n- Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n\n- Long-context Support up to 128K tokens and can generate up to 8K tokens.\n\n- Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).",
      "pricing": {
        "input": 0.0,
        "output": 0.0,
        "unit": "1M tokens"
      },
      "context_window": 32768,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768575"
    },
    {
      "id": "qwen/qwen-2.5-72b-instruct",
      "name": "Qwen2.5 72B Instruct",
      "provider": "openrouter",
      "description": "Qwen2.5 72B is the latest series of Qwen large language models. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\n\n- Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n\n- Long-context Support up to 128K tokens and can generate up to 8K tokens.\n\n- Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).",
      "pricing": {
        "input": 0.12,
        "output": 0.39,
        "unit": "1M tokens"
      },
      "context_window": 32768,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768576"
    },
    {
      "id": "openai/o1-preview",
      "name": "OpenAI",
      "provider": "openrouter",
      "description": "The latest and strongest model family from OpenAI, o1 is designed to spend more time thinking before responding.\n\nThe o1 models are optimized for math, science, programming, and other STEM-related tasks. They consistently exhibit PhD-level accuracy on benchmarks in physics, chemistry, and biology. Learn more in the [launch announcement](https://openai.com/o1).\n\nNote: This model is currently experimental and not suitable for production use-cases, and may be heavily rate-limited.",
      "pricing": {
        "input": 15.0,
        "output": 60.0,
        "unit": "1M tokens"
      },
      "context_window": 128000,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768577"
    },
    {
      "id": "openai/o1-preview-2024-09-12",
      "name": "OpenAI",
      "provider": "openrouter",
      "description": "The latest and strongest model family from OpenAI, o1 is designed to spend more time thinking before responding.\n\nThe o1 models are optimized for math, science, programming, and other STEM-related tasks. They consistently exhibit PhD-level accuracy on benchmarks in physics, chemistry, and biology. Learn more in the [launch announcement](https://openai.com/o1).\n\nNote: This model is currently experimental and not suitable for production use-cases, and may be heavily rate-limited.",
      "pricing": {
        "input": 15.0,
        "output": 60.0,
        "unit": "1M tokens"
      },
      "context_window": 128000,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768578"
    },
    {
      "id": "openai/o1-mini",
      "name": "OpenAI",
      "provider": "openrouter",
      "description": "The latest and strongest model family from OpenAI, o1 is designed to spend more time thinking before responding.\n\nThe o1 models are optimized for math, science, programming, and other STEM-related tasks. They consistently exhibit PhD-level accuracy on benchmarks in physics, chemistry, and biology. Learn more in the [launch announcement](https://openai.com/o1).\n\nNote: This model is currently experimental and not suitable for production use-cases, and may be heavily rate-limited.",
      "pricing": {
        "input": 1.1,
        "output": 4.4,
        "unit": "1M tokens"
      },
      "context_window": 128000,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768579"
    },
    {
      "id": "openai/o1-mini-2024-09-12",
      "name": "OpenAI",
      "provider": "openrouter",
      "description": "The latest and strongest model family from OpenAI, o1 is designed to spend more time thinking before responding.\n\nThe o1 models are optimized for math, science, programming, and other STEM-related tasks. They consistently exhibit PhD-level accuracy on benchmarks in physics, chemistry, and biology. Learn more in the [launch announcement](https://openai.com/o1).\n\nNote: This model is currently experimental and not suitable for production use-cases, and may be heavily rate-limited.",
      "pricing": {
        "input": 1.1,
        "output": 4.4,
        "unit": "1M tokens"
      },
      "context_window": 128000,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768580"
    },
    {
      "id": "cohere/command-r-plus-08-2024",
      "name": "Cohere",
      "provider": "openrouter",
      "description": "command-r-plus-08-2024 is an update of the [Command R+](/models/cohere/command-r-plus) with roughly 50% higher throughput and 25% lower latencies as compared to the previous Command R+ version, while keeping the hardware footprint the same.\n\nRead the launch post [here](https://docs.cohere.com/changelog/command-gets-refreshed).\n\nUse of this model is subject to Cohere's [Usage Policy](https://docs.cohere.com/docs/usage-policy) and [SaaS Agreement](https://cohere.com/saas-agreement).",
      "pricing": {
        "input": 2.5,
        "output": 10.0,
        "unit": "1M tokens"
      },
      "context_window": 128000,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768581"
    },
    {
      "id": "openai/gpt-4o-2024-08-06",
      "name": "OpenAI",
      "provider": "openrouter",
      "description": "The 2024-08-06 version of GPT-4o offers improved performance in structured outputs, with the ability to supply a JSON schema in the respone_format. Read more [here](https://openai.com/index/introducing-structured-outputs-in-the-api/).\n\nGPT-4o (\"o\" for \"omni\") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/models/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.\n\nFor benchmarking against other models, it was briefly called [\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)",
      "pricing": {
        "input": 2.5,
        "output": 10.0,
        "unit": "1M tokens"
      },
      "context_window": 128000,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768582"
    },
    {
      "id": "openai/gpt-4o-mini",
      "name": "OpenAI",
      "provider": "openrouter",
      "description": "GPT-4o mini is OpenAI's newest model after [GPT-4 Omni](/models/openai/gpt-4o), supporting both text and image inputs with text outputs.\n\nAs their most advanced small model, it is many multiples more affordable than other recent frontier models, and more than 60% cheaper than [GPT-3.5 Turbo](/models/openai/gpt-3.5-turbo). It maintains SOTA intelligence, while being significantly more cost-effective.\n\nGPT-4o mini achieves an 82% score on MMLU and presently ranks higher than GPT-4 on chat preferences [common leaderboards](https://arena.lmsys.org/).\n\nCheck out the [launch announcement](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) to learn more.\n\n#multimodal",
      "pricing": {
        "input": 0.15,
        "output": 0.6,
        "unit": "1M tokens"
      },
      "context_window": 128000,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768583"
    },
    {
      "id": "openai/gpt-4o-mini-2024-07-18",
      "name": "OpenAI",
      "provider": "openrouter",
      "description": "GPT-4o mini is OpenAI's newest model after [GPT-4 Omni](/models/openai/gpt-4o), supporting both text and image inputs with text outputs.\n\nAs their most advanced small model, it is many multiples more affordable than other recent frontier models, and more than 60% cheaper than [GPT-3.5 Turbo](/models/openai/gpt-3.5-turbo). It maintains SOTA intelligence, while being significantly more cost-effective.\n\nGPT-4o mini achieves an 82% score on MMLU and presently ranks higher than GPT-4 on chat preferences [common leaderboards](https://arena.lmsys.org/).\n\nCheck out the [launch announcement](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) to learn more.\n\n#multimodal",
      "pricing": {
        "input": 0.15,
        "output": 0.6,
        "unit": "1M tokens"
      },
      "context_window": 128000,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768585"
    },
    {
      "id": "anthropic/claude-3.5-sonnet-20240620:beta",
      "name": "Anthropic",
      "provider": "openrouter",
      "description": "Claude 3.5 Sonnet delivers better-than-Opus capabilities, faster-than-Sonnet speeds, at the same Sonnet prices. Sonnet is particularly good at:\n\n- Coding: Autonomously writes, edits, and runs code with reasoning and troubleshooting\n- Data science: Augments human data science expertise; navigates unstructured data while using multiple tools for insights\n- Visual processing: excelling at interpreting charts, graphs, and images, accurately transcribing text to derive insights beyond just the text alone\n- Agentic tasks: exceptional tool use, making it great at agentic tasks (i.e. complex, multi-step problem solving tasks that require engaging with other systems)\n\nFor the latest version (2024-10-23), check out [Claude 3.5 Sonnet](/anthropic/claude-3.5-sonnet).\n\n#multimodal",
      "pricing": {
        "input": 3.0,
        "output": 15.0,
        "unit": "1M tokens"
      },
      "context_window": 200000,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768586"
    },
    {
      "id": "anthropic/claude-3.5-sonnet-20240620",
      "name": "Anthropic",
      "provider": "openrouter",
      "description": "Claude 3.5 Sonnet delivers better-than-Opus capabilities, faster-than-Sonnet speeds, at the same Sonnet prices. Sonnet is particularly good at:\n\n- Coding: Autonomously writes, edits, and runs code with reasoning and troubleshooting\n- Data science: Augments human data science expertise; navigates unstructured data while using multiple tools for insights\n- Visual processing: excelling at interpreting charts, graphs, and images, accurately transcribing text to derive insights beyond just the text alone\n- Agentic tasks: exceptional tool use, making it great at agentic tasks (i.e. complex, multi-step problem solving tasks that require engaging with other systems)\n\nFor the latest version (2024-10-23), check out [Claude 3.5 Sonnet](/anthropic/claude-3.5-sonnet).\n\n#multimodal",
      "pricing": {
        "input": 3.0,
        "output": 15.0,
        "unit": "1M tokens"
      },
      "context_window": 200000,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768587"
    },
    {
      "id": "openai/gpt-4o",
      "name": "OpenAI",
      "provider": "openrouter",
      "description": "GPT-4o (\"o\" for \"omni\") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/models/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.\n\nFor benchmarking against other models, it was briefly called [\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)\n\n#multimodal",
      "pricing": {
        "input": 2.5,
        "output": 10.0,
        "unit": "1M tokens"
      },
      "context_window": 128000,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768588"
    },
    {
      "id": "openai/gpt-4o:extended",
      "name": "OpenAI",
      "provider": "openrouter",
      "description": "GPT-4o (\"o\" for \"omni\") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/models/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.\n\nFor benchmarking against other models, it was briefly called [\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)\n\n#multimodal",
      "pricing": {
        "input": 6.0,
        "output": 18.0,
        "unit": "1M tokens"
      },
      "context_window": 128000,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768589"
    },
    {
      "id": "openai/gpt-4o-2024-05-13",
      "name": "OpenAI",
      "provider": "openrouter",
      "description": "GPT-4o (\"o\" for \"omni\") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/models/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.\n\nFor benchmarking against other models, it was briefly called [\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)\n\n#multimodal",
      "pricing": {
        "input": 5.0,
        "output": 15.0,
        "unit": "1M tokens"
      },
      "context_window": 128000,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768590"
    },
    {
      "id": "cohere/command-r-plus",
      "name": "Cohere",
      "provider": "openrouter",
      "description": "Command R+ is a new, 104B-parameter LLM from Cohere. It's useful for roleplay, general consumer usecases, and Retrieval Augmented Generation (RAG).\n\nIt offers multilingual support for ten key languages to facilitate global business operations. See benchmarks and the launch post [here](https://txt.cohere.com/command-r-plus-microsoft-azure/).\n\nUse of this model is subject to Cohere's [Usage Policy](https://docs.cohere.com/docs/usage-policy) and [SaaS Agreement](https://cohere.com/saas-agreement).",
      "pricing": {
        "input": 3.0,
        "output": 15.0,
        "unit": "1M tokens"
      },
      "context_window": 128000,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768591"
    },
    {
      "id": "cohere/command-r-plus-04-2024",
      "name": "Cohere",
      "provider": "openrouter",
      "description": "Command R+ is a new, 104B-parameter LLM from Cohere. It's useful for roleplay, general consumer usecases, and Retrieval Augmented Generation (RAG).\n\nIt offers multilingual support for ten key languages to facilitate global business operations. See benchmarks and the launch post [here](https://txt.cohere.com/command-r-plus-microsoft-azure/).\n\nUse of this model is subject to Cohere's [Usage Policy](https://docs.cohere.com/docs/usage-policy) and [SaaS Agreement](https://cohere.com/saas-agreement).",
      "pricing": {
        "input": 3.0,
        "output": 15.0,
        "unit": "1M tokens"
      },
      "context_window": 128000,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768592"
    },
    {
      "id": "anthropic/claude-3-opus:beta",
      "name": "Anthropic",
      "provider": "openrouter",
      "description": "Claude 3 Opus is Anthropic's most powerful model for highly complex tasks. It boasts top-level performance, intelligence, fluency, and understanding.\n\nSee the launch announcement and benchmark results [here](https://www.anthropic.com/news/claude-3-family)\n\n#multimodal",
      "pricing": {
        "input": 15.0,
        "output": 75.0,
        "unit": "1M tokens"
      },
      "context_window": 200000,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768593"
    },
    {
      "id": "anthropic/claude-3-opus",
      "name": "Anthropic",
      "provider": "openrouter",
      "description": "Claude 3 Opus is Anthropic's most powerful model for highly complex tasks. It boasts top-level performance, intelligence, fluency, and understanding.\n\nSee the launch announcement and benchmark results [here](https://www.anthropic.com/news/claude-3-family)\n\n#multimodal",
      "pricing": {
        "input": 15.0,
        "output": 75.0,
        "unit": "1M tokens"
      },
      "context_window": 200000,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768594"
    },
    {
      "id": "mistralai/mistral-large",
      "name": "Mistral Large",
      "provider": "openrouter",
      "description": "This is Mistral AI's flagship model, Mistral Large 2 (version `mistral-large-2407`). It's a proprietary weights-available model and excels at reasoning, code, JSON, chat, and more. Read the launch announcement [here](https://mistral.ai/news/mistral-large-2407/).\n\nIt supports dozens of languages including French, German, Spanish, Italian, Portuguese, Arabic, Hindi, Russian, Chinese, Japanese, and Korean, along with 80+ coding languages including Python, Java, C, C++, JavaScript, and Bash. Its long context window allows precise information recall from large documents.",
      "pricing": {
        "input": 2.0,
        "output": 6.0,
        "unit": "1M tokens"
      },
      "context_window": 128000,
      "max_output": 4096,
      "release_date": "",
      "status": "ga",
      "features": [],
      "modalities": [
        "text"
      ],
      "use_cases": [],
      "training_cutoff": "",
      "last_updated": "2025-07-17T09:19:10.768596"
    }
  ]
}